{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark_cassandra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7f9e28262165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark_cassandra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# sc = pyspark.SparkConf()\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     .setMaster(\"local[*]\")\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark_cassandra'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark_cassandra\n",
    "# sc = pyspark.SparkConf()\\\n",
    "#     .setMaster(\"local[*]\")\\\n",
    "#     .set(\"spark.driver.memory\",\"8g\")\\\n",
    "#     .set(\"spark.executor.memory\",\"8g\")\\\n",
    "#     .set(\"spark.debug.maxToStringFields\", 10000) \\\n",
    "#     .setMaster(\"spark://spark-master:7077\") \\\n",
    "#     .set(\"spark.cassandra.connection.host\", \"cas-1\")\n",
    "# sparkContext = pyspark.SparkContext(conf=sc)\n",
    "# sc = CassandraSparkContext(conf=conf)\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .appName(\"pysaprk_python\")\\\n",
    "    .config('spark.cassandra.connection.host', 'localhost') \\\n",
    "    .config('spark.cassandra.connection.port', '9042') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from itertools import chain\n",
    "import redis, json, time\n",
    "myRedis = redis.Redis(host='127.0.0.1', port=6379, db=0)\n",
    "\n",
    "IP = \"127.0.0.1\"\n",
    "Port = 5559\n",
    "# cassandra 에서 받아오는 data schema\n",
    "schema = StructType([\n",
    "        StructField(\"created_at\", StringType(), False),\\\n",
    "        StructField(\"id\", LongType(), False),\n",
    "        StructField(\"truncated\", BooleanType(), False),\n",
    "        StructField(\"retweeted\", BooleanType(), False),\n",
    "        StructField(\"reply_status\", BooleanType(), False),\n",
    "        StructField(\"is_quote_status\", BooleanType(), False),\n",
    "        StructField(\"media_status\", BooleanType(), False),\n",
    "        StructField(\"text\", StringType(), False),\n",
    "        StructField(\"user\", StringType(), False),\n",
    "        StructField(\"retweeted_status\", StringType(), True),\n",
    "        StructField(\"quoted_status\", StringType(), True),\n",
    "        StructField(\"entities\", StringType(), False),\n",
    "        StructField(\"extended_entities\", StringType(), True),\n",
    "        StructField(\"extended_tweet\", StringType(), True),\n",
    "        StructField(\"lang\", StringType(), False),\n",
    "        StructField(\"date\", DateType(), False),\n",
    "        StructField(\"timestamp_ms\", TimestampType(), False)\n",
    "    ])\n",
    "\n",
    "myschema = [\n",
    "    'entities.hashtag.text as hashtag'\n",
    "]\n",
    "option_schema = [\n",
    "    'extended_entities.hashtag.text as extended_hashtag',\n",
    "    'quoted_status.entities.hashtag.text as quoted_hashtag'\n",
    "]\n",
    "# 분석 대상에서 제외할 단어 명시\n",
    "mystopwords = [\n",
    "    'RT',\n",
    "    'BTS',\n",
    "    'bts',\n",
    "    'Bts',\n",
    "    '방탄소년단'\n",
    "]\n",
    "# get DStream RDD\n",
    "def getStreaming(data, schema=None):\n",
    "    data.pprint() # 실시간으로 들어오는 tweet 출력\n",
    "    data.foreachRDD(process) # 각 rdd 별로 처리\n",
    "    return True\n",
    "\n",
    "# hashtag 전처리\n",
    "def hashtag_processing(text):\n",
    "    total = list(chain.from_iterable(text)) # 리스트 안에 리스트 하나의 리스트로 합치기\n",
    "    result = []\n",
    "    # 불용어 제거\n",
    "    for i in total:\n",
    "        if i not in mystopwords:\n",
    "            result.append(i)\n",
    "\n",
    "    print('응 들어옴')\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "# Convenience function for turning JSON strings into DataFrames.\n",
    "def process(rdd):\n",
    "    try:\n",
    "        # json 형식으로 들어온 data 중 필요한 데이터만 추출\n",
    "        data = json.loads(rdd)\n",
    "        print(data)\n",
    "        if data['truncated'] == 'true' and data['extended_entities']['hashtag']['text']:\n",
    "            myschema.append(option_schema[0])\n",
    "        if data['is_quoted_staus'] == 'true' and \\\n",
    "                data['quoted_status']['entities']['hashtag']['text']:\n",
    "            myschema.append((option_schema[1]))\n",
    "        print(myschema)\n",
    "\n",
    "        rawTweet = spark.read.json(rdd)\n",
    "        rawTweet.registerTempTable(\"tweets\") #creates an in-memory table that is scoped to the cluster in which it was created.\n",
    "        hashtag = rawTweet.selectExpr(myschema).rdd.flatMap(lambda x : x)\n",
    "        print(hashtag.collect())\n",
    "        # 현재 타임에 들어온 hashtag 전처리\n",
    "        result = hashtag_processing(hashtag.collect())\n",
    "\n",
    "        #word count 작업을 위해 결과 rdd로 만들어줌\n",
    "        rdd = spark.sparkContext.parallelize(result)\n",
    "        word_count(rdd)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 추출된 단어 word count\n",
    "def word_count(list):\n",
    "    print('word count 들어옴')\n",
    "    pairs = list.map(lambda word: (word, 1))\n",
    "    # 상위 10개만 가져오기 + 등장빈도 2번 이상\n",
    "    wordCounts = pairs.reduceByKey(lambda x, y: x + y).filter(lambda args : args[1] > 2)\n",
    "    ranking = wordCounts.takeOrdered(10, lambda args:-args[1])\n",
    "    print(ranking)\n",
    "    # key : 현재 시간 , value : 순위 결과 json 으로 redis 저장\n",
    "#     current_time = time.strftime(\"%d/%m/%Y\")\n",
    "#     rank_to_json = json.dumps(ranking)\n",
    "#     myRedis.set(current_time, rank_to_json, ex=60*5)\n",
    "\n",
    "ssc = StreamingContext(spark.SparkContext, 20)\n",
    "#lines = ssc.socketTextStream(IP, Port)\n",
    "#ssc.checkpoint(\"checkpoint\")\n",
    "lines = ssc.cassandraTable(\"test\",\"testing\")\n",
    "getStreaming(lines)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
